{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "### Loading Packages\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\nimport os, random\n\nimport gc\n\nimport time\n\nimport os.path\n\n\n# import warnings\n# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n# warnings.filterwarnings(\"ignore\", category=UserWarning)\n# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 2
        }, 
        {
            "source": "# I don't know why I don't have permissions to this file, but this seems to fix the issue\n!chmod 751 ~/notebook/work/Earthquake/train.csv", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 3
        }, 
        {
            "source": "cd ~/notebook/work/Earthquake", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "/gpfs/global_fs01/sym_shared/YPProdSpark/user/s55f-399db8dc89addb-a597b6337f26/notebook/work/Earthquake\n"
                }
            ], 
            "execution_count": 4
        }, 
        {
            "source": "## Loading the Data  \nLet's get a quick look at the file before we read it.  This csv file has a header for two columns of data, 'acoustic_data', and 'time_to_failure'.  If I loaded this we would have a dataframe 9.4 GB object with 629,145,480 rows of int64 and float64 data.  This is probably too much.  The head of the data shows that time is measured out to 10 digits so sampling the data seems reasonable approach, at least for exploring the data.  Casting the values to shorter length float and integer types seems to be a common approach to saving memory in other kernels, but it just ends up rounding the time to failure data to discrete interval blocks.  I'm going to assume that the data has been collected and stored at a deliberate sampling rate and not mess around with that.  For now, I will sample a certain percentage of the entire train dataset for a birdseye view of the data.  \n\nHere I also took a quick look at some of the test data which simply has one column 'acoustic_data'.  I'll try not to look at the test data too much so there is as little leakage as possible.  For now I'd just like enough information to be able to apply the same transformations to the test data as I do the training data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!head -10 train.csv", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "acoustic_data,time_to_failure\r\n12,1.4690999832\r\n6,1.4690999821\r\n8,1.469099981\r\n5,1.4690999799\r\n8,1.4690999788\r\n8,1.4690999777\r\n9,1.4690999766\r\n7,1.4690999755\r\n-5,1.4690999744\r\n"
                }
            ], 
            "execution_count": 5
        }, 
        {
            "source": "!head -10 seg_0012b5.csv", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "acoustic_data\r\n5\r\n8\r\n8\r\n7\r\n4\r\n1\r\n-1\r\n-4\r\n-1\r\n"
                }
            ], 
            "execution_count": 6
        }, 
        {
            "source": "def sample_csv(filename, nth_line):\n    \"\"\"\n    Reads every nth line of a csv file\n    If it is a training file, writes a csv file of this sampled data to 'filename_sample_' + nth_line+ 'nth.csv'\n    ex. if n = 100, every 100th line is read\n    and a 1% sample is taken and 'filename_sample_100th.csv' is written\n    https://stackoverflow.com/questions/22258491/read-a-small-random-sample-from-a-big-csv-file-into-a-python-data-frame\n    This function is slow so....\n    If a sampled csv file exists, loads that instead of the original filename \n    \"\"\"\n    sub_string = '.csv'\n    insert_text = '_sample_' + str(nth_line) + 'th'\n    index = filename.index(sub_string)\n    sample_filename = filename[:index] + insert_text + filename[index:]\n    if os.path.isfile(sample_filename):\n        df = pd.read_csv(sample_filename, header = 0)\n    else:\n        df = pd.read_csv(filename, header = 0, skiprows = lambda i: i % nth_line != 0)\n        # Write a sample file, but only for the large training file\n        if 'train' in filename:\n            df.to_csv(sample_filename, index=False)\n    return df", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 29
        }, 
        {
            "source": "def load_random_test(seed = None, verbose = False):\n    \"\"\"\n    load a random file in the current directory\n    reads csv\n    returns dataframe\n    \"\"\"\n    # Pick a random filename\n    random.seed(seed)\n    test_filename = random.choice(os.listdir())\n    \n    # Make sure you pick a test file\n    while (('train' or 'sample') in test_filename and 'seg' not in test_filename): \n        test_filename = random.choice(os.listdir())\n        \n    df = pd.read_csv(test_filename, header = 0)\n    \n    if verbose == True:\n        print('test filename: %s' % test_filename)\n    \n    return df", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 32
        }, 
        {
            "source": "%%time\nnth_line = 100\ntrain_filename = 'train.csv'\n    \ntrain_orig = sample_csv(train_filename, nth_line)\ntrain = train_orig.copy()\n\ntest_orig = load_random_test(seed = 42, verbose = True)\ntest = test_orig.copy()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "test filename: seg_8777bd.csv\nCPU times: user 1.61 s, sys: 144 ms, total: 1.75 s\nWall time: 1.75 s\n"
                }
            ], 
            "execution_count": 35
        }, 
        {
            "source": "train.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "   acoustic_data  time_to_failure\n0              9         1.469100\n1              5         1.469100\n2              0         1.469100\n3              1         1.469100\n4              1         1.469099", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acoustic_data</th>\n      <th>time_to_failure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9</td>\n      <td>1.469100</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>1.469100</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1.469100</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1.469100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1.469099</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 36, 
                    "metadata": {}
                }
            ], 
            "execution_count": 36
        }, 
        {
            "source": "train.describe()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "       acoustic_data  time_to_failure\ncount   6.291454e+06     6.291454e+06\nmean    4.520574e+00     5.678293e+00\nstd     1.051612e+01     3.672698e+00\nmin    -3.196000e+03     9.552926e-05\n25%     2.000000e+00     2.625997e+00\n50%     5.000000e+00     5.349798e+00\n75%     7.000000e+00     8.173372e+00\nmax     4.088000e+03     1.610740e+01", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acoustic_data</th>\n      <th>time_to_failure</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>6.291454e+06</td>\n      <td>6.291454e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.520574e+00</td>\n      <td>5.678293e+00</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.051612e+01</td>\n      <td>3.672698e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-3.196000e+03</td>\n      <td>9.552926e-05</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000e+00</td>\n      <td>2.625997e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.000000e+00</td>\n      <td>5.349798e+00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.000000e+00</td>\n      <td>8.173372e+00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>4.088000e+03</td>\n      <td>1.610740e+01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 37, 
                    "metadata": {}
                }
            ], 
            "execution_count": 37
        }, 
        {
            "source": "test.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "   acoustic_data\n0              6\n1              7\n2              1\n3              6\n4              7", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acoustic_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 38, 
                    "metadata": {}
                }
            ], 
            "execution_count": 38
        }, 
        {
            "source": "test.describe()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "       acoustic_data\ncount  150000.000000\nmean        4.255133\nstd         3.274337\nmin       -49.000000\n25%         2.000000\n50%         4.000000\n75%         6.000000\nmax        48.000000", 
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>acoustic_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>150000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.255133</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3.274337</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-49.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>48.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 39, 
                    "metadata": {}
                }
            ], 
            "execution_count": 39
        }, 
        {
            "source": "## Plotting the Overview  \nThe acoustic data seems to be some sort of waveform and the time to failure looks like a sawtooth pattern.  What we are actually looking at is a stream of data showing 16 different failures.  The time to failure is zero when a failure occurs.  Incidentally, the acoustic data shows significant local peaks when the time to failure is zero.  I don't know if these peaks define a failure or if this is a labeled dataset.  The time to failure peaks at a value corresponding to the time between failures.  The sawtooth graph we are seeing actually should be a discontinuous set of lines all of the same negative slope but with different heights.  \n\nThe median difference between the time_to_failure is $1.1*10^-7$ and given the 1% sampling rate of original dataset, this means that sampling period of the original dataset should be about $1.1*10^-9$ or about one nano-second.  It would also seem as if diff(time_to_failure) is pretty uniform across three quartiles and we should only expect to see it vary more than the sampling period the 16 times there is a failure and there is a jump in the time_to_failure.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def plot_acoustic_ttf(df, interval = None):\n    \"\"\"\n    Plots acoustic data and time_to_failure\n    interval specifies range in df to plot, default is the whole df\n    \"\"\"\n    if np.any(interval == None):\n        interval = [0, len(df)-1]\n    fig, ax1 = plt.subplots(figsize=(16, 8))\n    plt.title('acoustic_data and time_to_failure')\n    plt.plot(df['acoustic_data'].iloc[interval[0]:interval[1]], color = 'b')\n    ax1.set_ylabel('acoustic_data', color = 'b')\n    plt.legend(['acoustic_data'])\n    ax2 = ax1.twinx()\n    plt.plot(df['time_to_failure'].iloc[interval[0]:interval[1]], color = 'g')\n    ax2.set_ylabel('time_to_failure', color = 'g')\n    plt.legend(['time_to_failure'], loc = (0.875, 0.9))\n    plt.grid(False)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "plot_acoustic_ttf(train)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "train['time_to_failure'].diff().describe()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Locating Failures  \nLocating where the failures occur in the dataset should help our analysis.  First a new column 'failure' is added to the dataframe that is true when a failure occurs and false otherwise.  Hopefully a clearer picture emerges when we zoom in on the graph and plot when failures occur.  \n\nIt would be useful to look at each of these failures as seperate events by introducing new time variables: ts for the overall time since the experiment started, and ts_local for each of the failure cycles. \n\nWhat becomes clear when you zoom in on the graph is that the acoustic data has a large peak shortly before the failures occur.  I suppose that must mean that this dataset is somehow labelled for when failures occur.  At first glance, it would appear that the time between when this large and failures is about the same for all of the failure cycles.  The length of each cycle can vary but this interval between peak acoustic and failure seems uniform.\n\nThere may also be an increasing trend in average magnitude of acoustic data as the cycle time increases.\n\n ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def add_failure_column(df, threshold = 1):\n    \"\"\"\n    Locates where failures occur by finding local minima of time_to_failure\n    A local minima is found wherever the diff of consecutive values exceeds a threshold,\n    ie. where there is a jump\n    A new dataframe is returned with a new boolean column 'failure'\n    For test datasets where there is no time_to_failure, all values set to False\n    \"\"\"\n    # Turn off warning\n    pd.options.mode.chained_assignment = None\n    df['failure'] = False\n    if 'time_to_failure' in df.columns:\n        failures = df.loc[df['time_to_failure'].diff()>threshold].index-1\n        df['failure'].loc[failures] = True\n    return df", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "train = add_failure_column(train)\ntest = add_failure_column(test)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def add_time_columns(df):\n    \"\"\"\n    adds columns ts and ts_local\n    ts is time across the entire training set\n    ts_local resets to zero after each failure\n    IMPORTANT: The sampling frequency is not constant.  The recording device sends\n    packets of 4096 points of evenly spaced samples\n    and then puts a longer gap of time between packets.  \n    \"\"\"\n    # compute diff of time_to_failure\n    df['ts_diff'] = -df['time_to_failure'].diff().fillna(0)\n    \n    # need to get rid of this and just calculate things based off period\n    # set diff at failures to previous datapoint diff\n    failures = df.loc[df['failure']==True].index.tolist()\n    for failure in failures:\n        df['ts_diff'].iloc[failure+1] = df['ts_diff'].iloc[failure]\n    # set ts to the cumsum of all the ts_diff\n    df['ts'] = df['ts_diff'].cumsum()\n\n    \n    if 'time_to_failure' in df.columns:\n        # compute diff of time_to_failure\n        # somehow this loop resets the cumsum of ts_diff to zero after each failure\n        df['ts_diff'] = -df['time_to_failure'].diff().fillna(0)\n        ttf = [0] + failures\n        del ttf[-1]\n        for i in range(len(failures)):\n            df['ts_diff'].iloc[failures[i]+1] = -df['time_to_failure'].iloc[ttf[i]+1]\n        df['ts_local'] = df['ts_diff'].cumsum()\n\n        df.drop(['ts_diff'], axis = 1, inplace = True)\n    else:\n        df['ts_local'] = df['ts'].copy()\n    \n    return df", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "train = add_time_columns(train)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def plot_failure(df, column = 'acoustic_data', nrows=1, interval=None, figsize = (18,6), ylim = None):\n    \"\"\"\n    Plots column vs. failure, default is acoustic_data\n    nrows is the number of subplots\n    interval is an integer row index tuple or list for rows of data you wish to plot\n    \n    In case I forget why I used squeeze and 2-D axes arrays:\n    # https://stackoverflow.com/questions/19953348/error-when-looping-to-produce-subplots\n    \"\"\"\n\n    if np.any(interval == None):\n        interval = [0, len(df)-1]\n\n    df = df.iloc[interval[0]:interval[1]]\n    sub_length = int(len(df)/nrows)\n    fig, axes = plt.subplots(nrows,1, sharey= True, figsize = (figsize[0],figsize[1]*nrows), squeeze=False)\n    for i in range(0,nrows):\n        sub_range = range(i*sub_length,(i+1)*sub_length)\n        axes[i,0].plot( df['ts'].iloc[sub_range], df[column].iloc[sub_range], color = 'b')\n        axes[i,0].grid(False)\n        if ylim is not None:\n            axes[i,0].set_ylim(ylim)\n        for failure in df.iloc[sub_range].loc[df['failure']==True].index:\n            axes[i, 0].axvline(df['ts'].iloc[failure], color = 'r')\n            \n    return None", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "plot_failure(train, nrows = 4)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Identify Quality Issues  \n### Missing Values\nThere don't seem to be any null values in the data.  Keep in mind that this is a sample of only 1% of the data, but I suspect that there are no null values in the dataset.  If there are null values in acoustic_data or time_to_failure, filling in the values with interpolations of the adjacent valid data points would probably be a good strategy.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "train.isnull().sum()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Outliers  \nThe outliers in the acoustic_data are enormous compared to the rest of the data, about 1000 times as large.  The case could be made that outliers signal an imminent failure and need to be preserved, but they certainly would mess up training a linear model.  Some sort of data transformation must be done, either to get rid of outliers or perhaps a log transformation.  Anyways, it is usually good practice to have your data fit a gaussian distribution.  \nWhat is interesting about the data when plotted without outliers is that the median is about 5 and not zero.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Box Plot", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def plot_boxplot(df, show_outliers = True):\n    fig, ax = plt.subplots()\n    if show_outliers:\n        ax.set_title('acoustic_data')\n    else:\n        ax.set_title('acoustic_data, outliers excluded')\n    ax.boxplot(df['acoustic_data'], showfliers= show_outliers)\n    return None\n    ", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "plot_boxplot(train)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "plot_boxplot(train, show_outliers = False)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Filter Outliers", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def add_normal_outlier_columns(df, n_sigmas = 3, normal_alt_value = 0, outlier_alt_value = 0):\n    \"\"\"\n    adds the columns ad_normal and ad_outlier\n    corresponding to data that is either in the normal range or an outlier\n    n_sigma default is 3 corresponding to 99.7% of data being in the normal range\n    \"\"\"\n    df['normal'] = np.where(np.abs(df['acoustic_data']-df['acoustic_data'].mean()) \n                              <= (n_sigmas*df['acoustic_data'].std()), df['acoustic_data'], normal_alt_value)\n    df['outlier'] = np.where(np.abs(df['acoustic_data']-df['acoustic_data'].mean()) \n                              > (n_sigmas*df['acoustic_data'].std()), df['acoustic_data'], outlier_alt_value)\n    return df", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "train = add_normal_outlier_columns(train)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "#### Histogram", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# the histogram of the data\nn, bins, patches = plt.hist(train['normal'].fillna(0), bins = 60, density=1, facecolor='g', alpha=0.75)\n\n\nplt.xlabel('acoustic_data')\nplt.ylabel('Probability')\nplt.title('Histogram of acoustic_data without outliers')\nplt.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# the histogram of the data with outlier\nn, bins, patches = plt.hist(train['outlier'].fillna(0), log = True, density = 1, facecolor='g', alpha=0.75)\n\n\nplt.xlabel('acoustic_data')\nplt.ylabel('Counts')\nplt.title('Histogram of outlier acoustic_data, log-scale')\nplt.show()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "plot_failure(train, column = 'normal', nrows=1)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "plot_failure(train, column = 'outlier', nrows=1, ylim = (-500, 500))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Correllations  \nThere don't seem to be any promising correlations.  ts_local seems to have a good correlation with time_to_failure, but I just interpret this as saying failure is inevitable and the longer the experiment proceeds the greater the likelihood of failure.  Regardless, the test data doesn't have any time data so a model shouldn't be built around that.  I'll explore some rolling statistics to see if I can create features that correlate with time_to_failure", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "train.corrwith(train['time_to_failure'])", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Explore rolling statistics  \nBecause the data appears almost sinusoidal, analyzing the magnitude of the waveforms seems reasonable.  Typically you'd expect the mean across a large enough window of a sinusoidal signal to equal zero, in our case the mean is 5.  I decided to study the magnitude centered around the mean.  Because we don't want data from before a failure occured to affect analysis before the next failure, it seems prudent to divide up the dataset into 'episodes'.  In our case we have 16 failures, but 17 episodes since that last episode did not result in a recorded failure.  \nBecause what we call outlier data can immediately and drastically affect rolling mean of acoustic_data something must be done to filter the outliers.  What I've done is apply what is effectively a low-pass filter, allowing only signals that fall within +/- 3 sigmas of the mean.  Signals outside of this band are clipped.  This seemed to increase the magnitude of correlations.  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "train.describe()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def add_deviation_columns(df):\n    \"\"\"\n    Adds a deviation column: absolute_value(value-mean)\n    \"\"\"\n    for col in ['acoustic_data', 'normal', 'outlier']:\n        df[col+'_dev'] = np.abs(df[col]-df[col].mean())\n    return df", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "del(train)\ntrain = train_orig.copy()\ntrain = add_failure_column(train)\ntrain = add_time_columns(train)\ntrain = add_normal_outlier_columns(train)\ntrain = add_deviation_columns(train)\ntrain.head()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def get_episodes(df):\n    \"\"\"\n    Based on when failures occur, determines where episodes happen\n    \"\"\"\n    # Locate when failures occur\n    failures = df.loc[df['failure'] == True].index.tolist()\n\n    # Determine episode intervals\n    episodes = [range(0, failures[0]+1)]\n    for time in range(0,len(failures)-1):\n        episodes.append(range(failures[time]+1,failures[time+1]+1))\n    episodes.append(range(failures[-1]+1, len(df)))\n    \n    return episodes", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def add_rolling_mean_column(df, column = 'acoustic_data', windows = [100]):\n    \"\"\"\n    adds rolling mean data of a column with a rolling window size\n    does this for each episode in the training set, starting at the begining of each episode\n    uses the pandas series rolling function\n    otherwise this function mostly just labels the columns\n    \"\"\"\n    episodes = get_episodes(df)\n    for win in windows:\n        # Setup columns\n        new_col_name = column + '_mean_' + str(win)\n        rolling = []        \n        for episode in episodes:\n            series = df[column].iloc[episode]\n            # Call the rolling Function\n            rolling += (pd.Series(series).rolling(center = False, window = win).mean().tolist())\n        # write column to dataframe\n        df[new_col_name] = rolling\n    return df", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def add_rolling_std_column(df, column = 'acoustic_data', windows = [100]):\n    episodes = get_episodes(df)\n    for win in windows:\n        # Setup columns\n        new_col_name = column + '_std_' + str(win)\n        rolling = []        \n        for episode in episodes:\n            series = df[column].iloc[episode]\n            # Call the rolling Function\n            rolling += (pd.Series(series).rolling(center = False, window = win).std().tolist())\n        # write column to dataframe\n        df[new_col_name] = rolling\n    return df", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "def add_ewma_column(df, column = 'acoustic_data', center_of_mass = [100]):\n    episodes = get_episodes(df)\n    for center in center_of_mass:\n        # Setup columns\n        new_col_name = column + '_ewma_' + str(center)\n        rolling = []        \n        for episode in episodes:\n            series = df[column].iloc[episode]\n            # Call the rolling Function\n            rolling += (pd.Series(series).ewm(com = center).mean().tolist())\n        # write column to dataframe\n        df[new_col_name] = rolling\n    return df", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "%%time\nfor column in ['acoustic_data_dev', 'normal_dev', 'outlier_dev']:\n    windows = [100, 1000, 10000, 100000]\n    train = add_rolling_mean_column(train, column = column, windows = windows)\n    train = add_rolling_std_column(train, column = column, windows = windows)\n    train = add_ewma_column(train, column = column, center_of_mass = windows)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "train.columns", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "corr_acoustic = train[train.columns[10:22]].corrwith(train['time_to_failure'])\ncorr_acoustic.sort_values()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "corr_normal = train[train.columns[22:34]].corrwith(train['time_to_failure'])\ncorr_normal.sort_values()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "corr_outlier = train[train.columns[34:46]].corrwith(train['time_to_failure'])\ncorr_outlier.sort_values()", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "correlations = pd.DataFrame(pd.concat([corr_acoustic, corr_normal, corr_outlier]), columns = ['Correlation with time_to_failure'])\ncorrelations.sort_values(by = 'Correlation with time_to_failure', ascending = True)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "plot_failure(train, column = 'normal_dev_mean_10000', nrows=1, figsize = (18,3))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "plot_failure(train, column = 'normal_dev_ewma_10000', nrows=1, figsize = (18,3))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "## Fast Fourier Transform", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from scipy import fftpack", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Generate the signal", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "np.abs(train['time_to_failure'].diff().median())", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "# time_step is the difference between consecutive time_to_failure rows\ntime_step = np.abs(train['time_to_failure'].diff().median())\n\nepisode_num = 1\n\nepisodes = get_episodes(train)\nsig = train['acoustic_data'].iloc[episodes[episode_num]]\ntime_vec = train['ts_local'].iloc[episodes[episode_num]]\nplt.figure(figsize = (6,5))\nplt.plot(time_vec, sig, label = 'Original Signal')", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Compute and plot the power", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "%%time\n# The FFT of the signal\nsig_fft = fftpack.fft(sig)\n\n# And the power (sig_fft is of complex dtype)\npower = np.abs(sig_fft)\n\n# The corresponding frequencies\nsample_freq = fftpack.fftfreq(sig.size, d=time_step)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Plot the FFT power\nplt.figure(figsize=(6, 5))\nplt.plot(sample_freq, power)\nplt.xlabel('Frequency [Hz]')\nplt.ylabel('plower')\n\n# Find the peak frequency: we can focus on only the positive frequencies\npos_mask = np.where(sample_freq > 0)\nfreqs = sample_freq[pos_mask]\npeak_freq = freqs[power[pos_mask].argmax()]\n\n# Check that it does indeed correspond to the frequency that we generate\n# the signal with\n# np.allclose(peak_freq, 1./period)\n\n# An inner plot to show the peak frequency\naxes = plt.axes([0.55, 0.3, 0.3, 0.5])\nplt.title('Peak frequency')\nplt.plot(freqs[:8], power[pos_mask][:8])\nplt.setp(axes, yticks=[])\n\n# scipy.signal.find_peaks_cwt can also be used for more advanced\n# peak detection", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Besides 41 Hz, the most powerful frequencies are in the megahertz range\nfreqs.max()", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "upper_lim = 10\nplt.plot(freqs[:upper_lim], power[pos_mask][:upper_lim])\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## To Do:\n1. FFT  \n2. GRU network\n3. Wavelet", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}